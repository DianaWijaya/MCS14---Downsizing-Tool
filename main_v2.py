import streamlit as st
import os
import tempfile
import torch
from torch import nn
import time
import random
from mcunet.utils.pytorch_utils import count_net_flops, count_peak_activation_size

from pruning_logic.Pruning_definitions import get_model_size
from quantization_logic.quantization import quantize_model, get_tflite_model_size
from pruning_logic.Streamlined_prune import main_pruning_loop, knowledge_distillation_prune, main_finetune_model, test_model

# Set device
DEVICE = torch.device('cpu')  # Change to 'cuda' or 'mps' if available

def identify_model_blocks(model):
    """Extract model block names for selective pruning"""
    block_names = []
    try:
        for name, module in model.named_modules():
            if isinstance(module, nn.Module) and name.startswith("blocks"):
                parts = name.split(".")
                # Match only direct children like "blocks.0", "blocks.1", ...
                if len(parts) == 2 and parts[0] == "blocks" and parts[1].isdigit():
                    block_names.append(name)
        return sorted(block_names, key=lambda x: int(x.split(".")[1]))
    except Exception as e:
        st.error(f"Error extracting block names: {e}")
        return []

# App layout and styling
st.set_page_config(page_title="Model Optimizer", layout="wide")

# Custom CSS for better UI
st.markdown("""
<style>
    .main-header {text-align: center; margin-bottom: 2rem;}
    .optimization-section {background-color: #f5f5f5; padding: 20px; border-radius: 10px; margin-bottom: 20px;}
    .results-section {background-color: #f0f8ff; padding: 20px; border-radius: 10px;}
    .download-section {margin-top: 30px;}
</style>
""", unsafe_allow_html=True)

# Main application header
st.markdown("<h1 class='main-header'>Model Optimizer</h1>", unsafe_allow_html=True)

# Initialize session state variables for tracking UI state
if 'previous_mode' not in st.session_state:
    st.session_state.previous_mode = "Basic"

# Sidebar for upload and basic options
with st.sidebar:
    st.subheader("Model Upload")
    uploaded_file = st.file_uploader("Upload PyTorch model (.pth)", type=["pth"])
    
    if uploaded_file:
        st.success("âœ… Model uploaded")
        
        # Basic optimization methods selection
        st.subheader("Optimization Methods")
        optimization_mode = st.radio(
            "Select Mode:",
            ["Basic", "Advanced"]
        )

        st.markdown("""
            <hr style="border:1px solid #ccc; margin:20px 0;">
            """,
            unsafe_allow_html=True)
        
        # Check if mode changed and reset parameters if needed
        if 'previous_mode' in st.session_state and st.session_state.previous_mode != optimization_mode:
            if optimization_mode == "Advanced":
                # Reset to all unchecked when switching to Advanced
                st.session_state.block_pruning = False
                st.session_state.channel_pruning = False
                st.session_state.knowledge_distillation = False
                st.session_state.quantization = False
            st.session_state.previous_mode = optimization_mode
            
        # Update adv_tab_selected based on mode
        st.session_state.adv_tab_selected = (optimization_mode == "Advanced")
        
        if optimization_mode == "Basic":
            optimize_for = st.radio(
                "Optimize model for:",
                ["Size (Maximum Reduction)", "Speed (Balanced)", "Accuracy (Minimal Loss)"],
                index=1
            )
            
            # Set optimization parameters based on profile
            if optimize_for == "Size (Maximum Reduction)":
                st.session_state.block_pruning = True
                st.session_state.channel_pruning = True
                st.session_state.knowledge_distillation = False
                st.session_state.quantization = True
                st.session_state.quantization_type = "int8"
                st.session_state.block_pruning_ratio = 0.7
                st.session_state.channel_pruning_ratio = 0.6
                st.session_state.block_fine_tune = True
                st.session_state.channel_fine_tune = True
                st.session_state.block_fine_tune_epochs = 3
                st.session_state.channel_fine_tune_epochs = 3
                
            elif optimize_for == "Speed (Balanced)":
                st.session_state.block_pruning = True
                st.session_state.channel_pruning = True
                st.session_state.knowledge_distillation = False
                st.session_state.quantization = True
                st.session_state.quantization_type = "float16"
                st.session_state.block_pruning_ratio = 0.5
                st.session_state.channel_pruning_ratio = 0.4
                st.session_state.block_fine_tune = True
                st.session_state.channel_fine_tune = True
                st.session_state.block_fine_tune_epochs = 5
                st.session_state.channel_fine_tune_epochs = 5
                
            elif optimize_for == "Accuracy (Minimal Loss)":
                st.session_state.block_pruning = True
                st.session_state.channel_pruning = False
                st.session_state.knowledge_distillation = False
                st.session_state.quantization = True
                st.session_state.quantization_type = "dynamic"
                st.session_state.block_pruning_ratio = 0.3
                st.session_state.channel_pruning_ratio = 0.2
                st.session_state.block_fine_tune = True
                st.session_state.channel_fine_tune = True
                st.session_state.block_fine_tune_epochs = 10
                st.session_state.channel_fine_tune_epochs = 10
        
        # Only show advanced options if Advanced mode is selected
        if optimization_mode == "Advanced":
            # Initialize these state variables if they don't exist yet
            if 'block_pruning' not in st.session_state:
                st.session_state.block_pruning = False
            if 'channel_pruning' not in st.session_state:
                st.session_state.channel_pruning = False
            if 'knowledge_distillation' not in st.session_state:
                st.session_state.knowledge_distillation = False
            if 'quantization' not in st.session_state:
                st.session_state.quantization = False
            
            # Advanced optimization method selection
            st.session_state.block_pruning = st.checkbox("Block Level Pruning", value=st.session_state.block_pruning)
            st.session_state.channel_pruning = st.checkbox("Channel Pruning", value=st.session_state.channel_pruning)
            st.session_state.knowledge_distillation = st.checkbox("Knowledge Distillation", value=st.session_state.knowledge_distillation)
            st.session_state.quantization = st.checkbox("Quantization", value=st.session_state.quantization)

# Main content
if uploaded_file:
    # Save uploaded file temporarily
    with tempfile.NamedTemporaryFile(delete=False, suffix='.pth') as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        model_path = tmp_file.name
        model = torch.load(model_path, map_location=DEVICE, weights_only=False)
    
    # Main content area - only shown after model is uploaded
    with st.container():
        st.markdown("<div class='optimization-section'>", unsafe_allow_html=True)
        
        # Initialize block_pruning_ratios at the top level for both Basic and Advanced modes
        blocks = identify_model_blocks(model)
        block_pruning_ratios = {}
        
        # Advanced Options for fine-tuning the optimization - only show when Advanced tab is selected
        if st.session_state.adv_tab_selected:
            with st.expander("Optimization Parameters", expanded=True):
                # Create a list of selected tabs in a specific order
                tab_names = []
                
                # Always add pruning tabs first if selected
                if st.session_state.block_pruning:
                    tab_names.append("Block Pruning")
                if st.session_state.channel_pruning:
                    tab_names.append("Channel Pruning")
                    
                # Always add fine-tuning if any pruning is selected
                if st.session_state.block_pruning or st.session_state.channel_pruning:
                    tab_names.append("Fine-tuning")
                    
                # Then add other options
                if st.session_state.knowledge_distillation:
                    tab_names.append("Knowledge Distillation")
                if st.session_state.quantization:
                    tab_names.append("Quantization")
                
                # Only create tabs if we have optimization methods selected
                if tab_names:
                    param_tabs = st.tabs(tab_names)
                    
                    # Create a tab index dictionary for direct access
                    tab_indices = {name: idx for idx, name in enumerate(tab_names)}
                    
                    # Block pruning settings
                    if st.session_state.block_pruning:
                        with param_tabs[tab_indices["Block Pruning"]]:
                            use_same_ratio = st.checkbox("Use same pruning ratio for all blocks", value=True)
                            
                            if use_same_ratio:
                                if 'block_pruning_ratio' not in st.session_state:
                                    st.session_state.block_pruning_ratio = 0.5
                                
                                st.session_state.block_pruning_ratio = st.slider(
                                    "Block Pruning Ratio", 0.0, 0.9, 
                                    st.session_state.block_pruning_ratio, 
                                    0.05
                                )
                                for block in blocks:
                                    block_pruning_ratios[block] = st.session_state.block_pruning_ratio
                            else:
                                cols = st.columns(2)
                                for i, block in enumerate(blocks):
                                    with cols[i % 2]:
                                        block_pruning_ratios[block] = st.slider(
                                            f"Block {block.split('.')[1]}", 
                                            0.0, 0.9, 0.5, 0.05,
                                            key=f"block_{block}"
                                        )
                    
                    # Channel pruning settings
                    if st.session_state.channel_pruning:
                        with param_tabs[tab_indices["Channel Pruning"]]:
                            if 'channel_pruning_ratio' not in st.session_state:
                                st.session_state.channel_pruning_ratio = 0.4
                                
                            st.session_state.channel_pruning_ratio = st.slider(
                                "Channel Pruning Ratio", 0.0, 0.9, 
                                st.session_state.channel_pruning_ratio,
                                0.05
                            )
                            
                            st.info("Channel pruning uniformly reduces model width across all layers.")
                    
                    # Fine-tuning settings - only show if any pruning is selected
                    if st.session_state.block_pruning or st.session_state.channel_pruning:
                        with param_tabs[tab_indices["Fine-tuning"]]:
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                if st.session_state.block_pruning:
                                    if 'block_fine_tune' not in st.session_state:
                                        st.session_state.block_fine_tune = True
                                    if 'block_fine_tune_epochs' not in st.session_state:
                                        st.session_state.block_fine_tune_epochs = 5
                                        
                                    st.session_state.block_fine_tune = st.checkbox("Enable fine-tuning after block pruning", value=st.session_state.block_fine_tune)
                                    if st.session_state.block_fine_tune:
                                        st.session_state.block_fine_tune_epochs = st.slider(
                                            "Block Pruning Fine-tuning Epochs", 1, 20, 
                                            st.session_state.block_fine_tune_epochs
                                        )
                            
                            with col2:
                                if st.session_state.channel_pruning:
                                    if 'channel_fine_tune' not in st.session_state:
                                        st.session_state.channel_fine_tune = True
                                    if 'channel_fine_tune_epochs' not in st.session_state:
                                        st.session_state.channel_fine_tune_epochs = 5
                                        
                                    st.session_state.channel_fine_tune = st.checkbox("Enable fine-tuning after channel pruning", value=st.session_state.channel_fine_tune)
                                    if st.session_state.channel_fine_tune:
                                        st.session_state.channel_fine_tune_epochs = st.slider(
                                            "Channel Pruning Fine-tuning Epochs", 1, 20, 
                                            st.session_state.channel_fine_tune_epochs
                                        )
                            
                            st.info("Fine-tuning helps restore accuracy after pruning by retraining the model.")
                    
                    # Knowledge Distillation Settings
                    if st.session_state.knowledge_distillation:
                        with param_tabs[tab_indices["Knowledge Distillation"]]:
                            teacher_model_file = st.file_uploader("Upload Teacher Model (.pth)", type=["pth"], key="teacher_model")
                            distillation_epochs = st.slider("Distillation Epochs", 1, 20, 10)
                            
                            st.info("Knowledge distillation transfers knowledge from a larger teacher model to your pruned model.")
                    
                    # Quantization settings
                    if st.session_state.quantization:
                        with param_tabs[tab_indices["Quantization"]]:
                            if 'quantization_type' not in st.session_state:
                                st.session_state.quantization_type = "float16"
                                
                            st.session_state.quantization_type = st.selectbox(
                                "Quantization Type",
                                ["int8", "float16", "dynamic"],
                                index=["int8", "float16", "dynamic"].index(st.session_state.quantization_type)
                            )
                            
                            # Add descriptions for each quantization type
                            if st.session_state.quantization_type == "int8":
                                st.info("INT8: Maximum size reduction. Converts weights and activations to 8-bit integers.")
                            elif st.session_state.quantization_type == "float16":
                                st.info("FLOAT16: Good balance between size and accuracy. Uses 16-bit floating point values.")
                            elif st.session_state.quantization_type == "dynamic":
                                st.info("DYNAMIC: Best accuracy preservation. Dynamic range quantization at runtime.")
                else:
                    st.info("Please select optimization methods in the Advanced tab to configure parameters.")
        
        # For Basic mode, set the block_pruning_ratios using the global block_pruning_ratio
        elif st.session_state.block_pruning:
            # Apply the same ratio to all blocks
            for block in blocks:
                block_pruning_ratios[block] = st.session_state.block_pruning_ratio
        
        st.markdown("</div>", unsafe_allow_html=True)
        
        # Process the model
        if st.button("Optimize Model", type="primary"):
            # Check if any optimization method is selected
            if not (st.session_state.block_pruning or st.session_state.channel_pruning or 
                    st.session_state.knowledge_distillation or st.session_state.quantization):
                st.error("Please select at least one optimization method before proceeding.")
            else:
                with st.spinner("Optimizing your model..."):
                    try:
                       # Calculate original model stats
                        dummy_input = (1, 1, 96, 96)
                        original_params = sum(p.numel() * p.element_size() for p in model.parameters())
                        original_size_mb = get_model_size(model)
                        original_flops = count_net_flops(model, dummy_input)
                        original_peak_act = count_peak_activation_size(model, dummy_input)

                        # For demonstration, we're using random accuracy, but in production this would be:
                        # original_acc = test_model(model, DEVICE)
                        original_acc = random.randint(80, 95)  # For demo purposes

                        # Create progress tracking
                        progress_bar = st.progress(0)
                        status_text = st.empty()

                        # Initialize the model to be optimized
                        current_model = model
                        progress_value = 0
                        progress_increment = 100 / (sum([st.session_state.block_pruning, 
                                                        st.session_state.channel_pruning, 
                                                        st.session_state.knowledge_distillation, 
                                                        st.session_state.quantization]) or 1)

                        # Determine pruning type
                        pruning_type = ""
                        if st.session_state.block_pruning and st.session_state.channel_pruning:
                            pruning_type = "BOTH"
                        elif st.session_state.block_pruning:
                            pruning_type = "BLOCK"
                        elif st.session_state.channel_pruning:
                            pruning_type = "UNIFORM"

                        # Apply pruning if selected
                        if st.session_state.block_pruning or st.session_state.channel_pruning:
                            status_text.text("Applying pruning...")
                            
                            current_model = main_pruning_loop(
                                model=current_model,
                                block_level_dict=block_pruning_ratios if st.session_state.block_pruning else {},
                                uniform_pruning_ratio=st.session_state.channel_pruning_ratio if st.session_state.channel_pruning else 0.0,
                                block_fine_tune_epochs=st.session_state.block_fine_tune_epochs if st.session_state.block_fine_tune else 0,
                                channel_fine_tune_epochs=st.session_state.channel_fine_tune_epochs if st.session_state.channel_fine_tune else 0,
                                device=DEVICE,
                                type=pruning_type
                            )
                            
                            progress_value += progress_increment
                            progress_bar.progress(min(progress_value / 100, 1.0))

                        distilled_model = current_model
                        
                        # Apply knowledge distillation if selected
                        if st.session_state.knowledge_distillation:
                            status_text.text("Applying knowledge distillation...")
                            
                            # Initialize teacher_model_file to avoid NameError
                            teacher_model_file = None
                            
                            # Check for teacher model in session state or widgets
                            if 'teacher_model' in st.session_state:
                                teacher_model_file = st.session_state.teacher_model
                            
                            if not teacher_model_file:
                                st.error("Teacher model file not uploaded, but Knowledge Distillation was selected.")
                                st.stop()
                            
                            st.write(f"Starting Knowledge Distillation for {st.session_state.get('distillation_epochs', 5)} epochs...")
                            
                            # Load teacher model
                            teacher_model_path_tmp = ""
                            with tempfile.NamedTemporaryFile(delete=False, suffix='.pth') as tmp_teacher_file:
                                tmp_teacher_file.write(teacher_model_file.getvalue())
                                teacher_model_path_tmp = tmp_teacher_file.name
                            
                            try:
                                # Load teacher model
                                teacher_model = torch.load(teacher_model_path_tmp, map_location=DEVICE)
                                teacher_model.to(DEVICE)
                                teacher_model.eval()
                                
                                # Student model is pruned_model at this point
                                distilled_model.to(DEVICE)
                                
                                # Call distillation
                                distilled_model = knowledge_distillation_prune(
                                    teacher_model=teacher_model,
                                    student_model=distilled_model,
                                    num_epochs=st.session_state.get('distillation_epochs', 5),
                                    device=DEVICE
                                )
                                
                                st.success("Knowledge Distillation complete.")
                            except Exception as e:
                                st.error(f"Error during knowledge distillation: {str(e)}")
                            finally:
                                # Clean up temporary file
                                if os.path.exists(teacher_model_path_tmp):
                                    os.unlink(teacher_model_path_tmp)
                            
                            progress_value += progress_increment
                            progress_bar.progress(min(progress_value / 100, 1.0))

                        # Save the pruned model
                        pruned_model = current_model
                        pruned_params = sum(p.numel() * p.element_size() for p in pruned_model.parameters())
                        pruned_size_mb = get_model_size(pruned_model)
                        pruned_flops = count_net_flops(pruned_model, dummy_input)
                        pruned_peak_act = count_peak_activation_size(pruned_model, dummy_input)

                        # For demonstration, we'd normally use:
                        # pruned_acc = test_model(pruned_model, DEVICE)

                        # But instead we'll simulate a slight accuracy drop for demo
                        pruned_acc = max(original_acc - random.randint(0, 10), 0)

                        # Save the full model for potential quantization
                        pruned_model_path = os.path.join(tempfile.gettempdir(), "pruned_model.pth")
                        torch.save(pruned_model, pruned_model_path)

                        # Save state dict for download
                        pruned_state_dict_path = os.path.join(tempfile.gettempdir(), "pruned_model_state_dict.pth")
                        torch.save(pruned_model.state_dict(), pruned_state_dict_path)

                        # Apply quantization if selected
                        quantized_model_path = None
                        quantized_size = None
                        if st.session_state.quantization:
                            status_text.text(f"Applying {st.session_state.quantization_type} quantization...")
                            
                            # Create a temporary directory for the quantized model
                            quantized_model_path = os.path.join(tempfile.gettempdir(), "quantized_model.tflite")
                            
                            try:
                                # Apply quantization using the imported function
                                quantize_model(pruned_model_path, quantized_model_path, "person_detection_validation", st.session_state.quantization_type)
                                
                                # Calculate size of quantized model
                                quantized_size = get_tflite_model_size(quantized_model_path)
                            except Exception as e:
                                st.error(f"Error during quantization: {str(e)}")
                                quantized_model_path = None
                            
                            progress_value += progress_increment
                            progress_bar.progress(1.0)

                        # Complete
                        status_text.text("Optimization complete!")

                        # Calculate final metrics
                        final_flops = pruned_flops  # FLOPS don't change with quantization
                        final_model = pruned_model

                        # Display results
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.markdown("### Before")
                            st.metric("Size", f"{original_size_mb:.2f} MB")
                            st.metric("FLOPs", f"{original_flops / 1e6:.2f} M")
                            st.metric("Accuracy", f"{original_acc}%")
                        
                        with col2:
                            st.markdown("### After")
                            # Calculate size reduction based on whether quantization was applied
                            size_reduction = ((original_size_mb - pruned_size_mb) / original_size_mb) * 100
                            
                            if st.session_state.quantization and quantized_size is not None:
                                final_size = quantized_size
                                quantized_size_reduction = ((original_size_mb - quantized_size) / original_size_mb) * 100
                                size_display = f"{quantized_size:.2f} MB"  
                                size_reduction_display = f"-{quantized_size_reduction:.1f}%"
                            else:
                                final_size = pruned_size_mb
                                size_display = f"{pruned_size_mb:.2f} MB"
                                size_reduction_display = f"-{size_reduction:.1f}%"
                                
                            st.metric("Size", size_display, delta=size_reduction_display, delta_color="inverse")
                            
                            flops_reduction = ((original_flops - final_flops) / original_flops) * 100
                            st.metric("FLOPs", f"{final_flops / 1e6:.2f} M", 
                                     delta=f"-{flops_reduction:.1f}%", delta_color="inverse")
                            
                            acc_delta = pruned_acc - original_acc
                            st.metric("Accuracy", f"{pruned_acc}%", 
                                     delta=f"{acc_delta:.1f}%", delta_color="normal")
                        
                        # Summary
                        methods_used = []
                        if st.session_state.block_pruning: 
                            methods_used.append(f"Block Pruning{' with fine-tuning' if st.session_state.block_fine_tune else ''}")
                        if st.session_state.channel_pruning: 
                            methods_used.append(f"Channel Pruning{' with fine-tuning' if st.session_state.channel_fine_tune else ''}")
                        if st.session_state.knowledge_distillation: 
                            methods_used.append("Knowledge Distillation")
                        if st.session_state.quantization: 
                            methods_used.append(f"{st.session_state.quantization_type.upper()} Quantization")
                        
                        st.info(f"Methods used: {', '.join(methods_used)}")
                        
                        # Download section
                        st.markdown("<div class='download-section'>", unsafe_allow_html=True)
                        st.subheader("Download Optimized Models")
                        
                        # Store files in session_state
                        with open(pruned_state_dict_path, "rb") as f:
                            pruned_bytes = f.read()
                        
                        download_col1, download_col2 = st.columns(2)
                        
                        with download_col1:
                            st.download_button(
                                label="Download Pruned Model (.pth)",
                                data=pruned_bytes,
                                file_name="optimized_model.pth",
                                mime="application/octet-stream"
                            )
                        
                        if st.session_state.quantization and quantized_model_path and os.path.exists(quantized_model_path):
                            with open(quantized_model_path, "rb") as f:
                                quantized_bytes = f.read()
                            
                            with download_col2:
                                st.download_button(
                                    label="Download Quantized Model (.tflite)",
                                    data=quantized_bytes,
                                    file_name=f"optimized_model_{st.session_state.quantization_type}.tflite",
                                    mime="application/octet-stream"
                                )
                        
                        st.markdown("</div>", unsafe_allow_html=True)
                        st.markdown("</div>", unsafe_allow_html=True)
                    
                    except Exception as e:
                        st.error(f"Error during optimization: {str(e)}")
                        
                    # Clean up temporary files
                    if os.path.exists(model_path):
                        os.unlink(model_path)
else:
    # Initial page content when no model is uploaded
    st.info("ðŸ‘ˆ Upload your PyTorch model in the sidebar to get started")
    
    # Application information
    with st.expander("About Model Optimizer", expanded=True):
        st.markdown("""
        This tool helps you optimize deep learning models for deployment on resource-constrained devices. It provides:
        
        - **Block and Channel Pruning**: Remove unnecessary parts of your model
        -  **Knowledge Distillation**: Transfer knowledge from a larger teacher model
        - **Quantization**: Convert to smaller numerical formats
        - **Fine-tuning**: Restore accuracy after optimization
        
        Start by uploading your PyTorch model in the sidebar.
        """)